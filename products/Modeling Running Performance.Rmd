---
title: "Modeling Run Performance"
author: "Joe Martin"
date: "11/26/2021"
output: pdf_document
---
## Garmin Data Modeling

```{r echo=FALSE}
# Load packages and data
pacman::p_load(tidyverse, tidymodels, here)

garmin <- read_rds(here::here("data","processed_data", "garmin_data.rds"))

# Transform data to have only the necessary variables
df <- garmin %>% select(-id, -week, -calories) %>% drop_na()
```

The two most obvious primary target variables are average speed (avg_spd) in miles per hour, and average pace (avg_pace_sec) in seconds. A higher average speed and a lower average pace are the desired outcome when measuring performance over time. Reviewing the results of the two preliminary linear regression models, the more desirable variable is average pace, as it has stronger relationships with other variables. 

```{r}
# Create preliminary test
prelim_spd <- lm(avg_spd ~ ., df)
summary(prelim_spd)
```

```{r}
prelim_pace <- lm(avg_pace_sec ~ ., df)
summary(prelim_pace)
```

The ultimate goal of this model is to utilize data leading up to a performance event. Thinking about the purpose of the model (predicting how well I can perform given a set of racing conditions), the best target variable to choose is Average Pace (using only seconds as the unit). This variable is easier to work with than total time (which is in an HMS format) while having the same outcome. It is also something I can know in real-time on runs through my watch and has actionable meaning, compared to the average speed variable. Going forward, all models will use average pace (in seconds) as the target variable and use a linear regression for prediction.

```{r}
set.seed(456)
# Split data into training and testing sets
df_split <- initial_split(df, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
pace_rec <- recipe(avg_pace_sec ~ ., data = train_df)

summary(pace_rec)
```

```{r}
lm_pace <- linear_reg() %>%
  set_engine("lm")

pace_wflow <- workflow()%>%
  add_model(lm_pace) %>%
  add_recipe(pace_rec)

pace_fit <- pace_wflow %>% 
  fit(data = train_df) 

tidy(pace_fit)
```

```{r}
predict(pace_fit, test_df)
```

```{r}
pace_aug <- augment(pace_fit, test_df)

pace_aug %>% select(avg_pace_sec, .pred)
```

The R Mean-Squared Error for this model is 5.24. In other words, this model can predict average pace within 5.24 seconds.

```{r}
pace_error <- pace_aug %>% 
  rmse(truth = avg_pace_sec, .pred)

pace_error
```

## Logistic Regression

### All Variables

This first logistic regression is meant to predict whether an activity highly impacts aerobic training. This variable is relevant because it is the highest measure for aerobic conditioning without being over-reaching. In this analysis, calories and other variables related to aerobic training effect were removed. 
```{r}
set.seed(456)

df2 <- df %>% mutate(high_impact = ifelse(aerobic_fct == "Highly Impacting", 1,0))
df2$high_impact <- factor(df2$high_impact)

# Split data into training and testing sets
df2_split <- initial_split(df2, prop = 3/4)

train_df2 <- training(df2_split)
test_df2 <- testing(df2_split)

# Create recipe. Use all variables except aerobic_TE and related
aerobic_rec <- recipe(high_impact ~ short_distance + middle_distance + long_distance + 
                        max_spd + avg_spd + anaerobic_value + `sweat_loss(ml)` + best_pace_sec +
                        avg_pace_sec + max_elevation + min_elevation + avg_stride +
                        total_decent + total_ascent + max_run_cadence + avg_run_cadence + 
                        max_hr + avg_hr + distance, data = train_df2)

summary(aerobic_rec)
```

```{r}
log_reg <- logistic_reg() %>%
  set_engine("glm")

aero_wkfl <- workflow()%>%
  add_model(log_reg) %>%
  add_recipe(aerobic_rec)

aero_fit <- aero_wkfl %>% 
  fit(data = train_df2) 

tidy(aero_fit)
```

```{r}
predict(aero_fit, test_df2)
```

```{r}
aero_aug <- augment(aero_fit, test_df2)

aero_aug %>% select(high_impact, .pred_class)
```

```{r}
aero_aug$.pred_class <- as.character(aero_aug$.pred_class)
aero_aug$.pred_class <- as.numeric(aero_aug$.pred_class)

aero_aug %>% 
  roc_curve(truth = high_impact, .pred_class, event_level="second") %>%
  autoplot()
```

```{r}
aero_aug %>% 
  roc_auc(truth = high_impact, .pred_class, event_level="second")
```

Both of these models are acceptable for predicting performance. In the scope of this project, it seems that average pace is the best target variable to choose. When this model is later deployed, being able to predict average pace means being able to predict how well I would perform given a set of recent runs and the temperature and elevation of a location. 

These analyses provide a good starting point for building a more complex model that can predict good performance. The possible next step is to use k-fold cross validation to enhance the quality of my training set. In this section, the random forest model will use k-fold cross validation and train with all variables.
```{r}
pacman::p_load(tidymodels, ranger, parallel)

cores <- parallel::detectCores()

set.seed(456)

# Split data into training and testing sets
df_split <- initial_split(df, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
rf_rec <- recipe(avg_pace_sec ~ ., data = train_df) %>%
          step_dummy(all_nominal_predictors())

folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_pace_sec)

summary(rf_rec)
```

```{r}
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_res <- rf_wf %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res %>%
  show_best(metric = "rmse")

autoplot(rf_res)
```

```{r}
rf_best <- rf_res %>%
  select_best(metric = "rmse")
rf_res %>% collect_predictions()
```

```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(rf_best)

final_fit_rf <- final_rf_wf %>%
  last_fit(df_split)

final_fit_rf %>% collect_metrics()

rf_rmse <- 
  rf_res %>%
  collect_predictions(parameters = rf_best) %>%
  rmse(avg_pace_sec, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse
```

This model predicts average pace within 4.4 seconds. In an attempt to improve the accuracy, some variables will be eliminated. This model ran with 21 predictors. Thinking about the purpose of this model (predicting potential race pace), there are features in this dataset which might not be available before the race. For example, speed would be related to the target (average pace) and not necessarily available as a predictor. The same goes for sweat loss. Therefore, these features will be removed. 

```{r}
set.seed(456)

#get rid of max_hr,min_elevation, max_elevation, and sweat_loss
df1 <- df %>% select(-max_spd, -avg_spd, -`sweat_loss(ml)`)

# Split data into training and testing sets
df1_split <- initial_split(df1, prop = 3/4)

train_df1 <- training(df1_split)
test_df1 <- testing(df1_split)

# Create recipe
rf_rec <- recipe(avg_pace_sec ~ ., data = train_df1) %>%
          step_dummy(all_nominal_predictors())

folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_hr)

summary(rf_rec)
```

```{r}
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_res <- rf_wf %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res %>%
  show_best(metric = "rmse")

autoplot(rf_res)
```

```{r}
rf_best <- rf_res %>%
  select_best(metric = "rmse")
rf_res %>% collect_predictions()
```

```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(rf_best)

final_fit_rf <- final_rf_wf %>%
  last_fit(df1_split)

final_fit_rf %>% collect_metrics()

rf_rmse <- 
  rf_res %>%
  collect_predictions(parameters = rf_best) %>%
  rmse(avg_pace_sec, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse
```

The accuracy (measured by RMSE) got worse (7.11). Try dropping more variables. This time, ascent, descent, aerobic factors and anaerobic factors. 


Try running the model with tuned parameters.
```{r}
set.seed(456)

# Split data into training and testing sets
df1_split <- initial_split(df1, prop = 3/4)

train_df1 <- training(df1_split)
test_df1 <- testing(df1_split)

# Create recipe
rf_rec <- recipe(avg_pace_sec ~ ., data = train_df1) %>%
          step_dummy(all_nominal_predictors())

folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_hr)

summary(rf_rec)
```

```{r}
rf_mod <- rand_forest(mtry = 7, min_n = 7, trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_res <- rf_wf %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res %>%
  show_best(metric = "rmse")
```

```{r}
rf_best <- rf_res %>%
  select_best(metric = "rmse")
rf_res %>% collect_predictions()
```

```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(rf_best)

final_fit_rf <- final_rf_wf %>%
  last_fit(df1_split)

final_fit_rf %>% collect_metrics()

rf_rmse <- 
  rf_res %>%
  collect_predictions(parameters = rf_best) %>%
  rmse(avg_pace_sec, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse
```

This final Random Forest model has a greater RMSE value than the previous one. One of the concerns with the dataset is the greater number of features (21 total predictors avialable). LASSO may be a good option to automate feature selection. 

```{r}
set.seed(456)
# Split data into training and testing sets
df_split <- initial_split(df1, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
lasso_rec <- recipe(avg_pace_sec ~ ., data = train_df)

# create folds
folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_hr)

summary(lasso_rec)
```

```{r}
lasso_mod <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("lm")

lasso_wkfl <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(lasso_rec)
```

```{r}
# create penalty grid for tuning
lasso_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

# lowest penalties
lasso_grid %>% top_n(-5)

#highest penalties
lasso_grid %>% top_n(5)
```

```{r}
lasso_res <- lasso_wkfl %>%
  tune_grid(folds,
            grid = lasso_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
top_models <- lasso_res %>%
  show_best("rmse")
top_models
```

```{r}
lasso_best <- lasso_res %>%
  select_best("rmse")
lasso_best

final_lasso_wf <- lasso_wkfl %>%
  finalize_workflow(lasso_best)

final_lasso_fit <- final_lasso_wf %>%
  last_fit(df_split)

final_lasso_fit %>% collect_metrics()
```

The LASSO model does improve the RMSE and is likely the best path forward. The next steps in this project will be to further tune this model. 