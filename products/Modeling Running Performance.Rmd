---
title: "Modeling Run Performance"
author: "Joe Martin"
date: "10/27/2021"
output: pdf_document
---
## Garmin Data Modeling

```{r echo=FALSE}
# Load packages and data
pacman::p_load(tidyverse, tidymodels, here)

garmin <- read_rds(here::here("data","processed_data", "garmin_data.rds"))

# Transform data to have only the necessary variables
df <- garmin %>% select(-id,-week, -calories) %>% drop_na()
```

The two most obvious primary target variables are average speed (avg_spd) in miles per hour, and average pace (avg_pace_sec) in seconds. A higher average speed and a lower average pace are the desired outcome when measuring performance over time. Reviewing the results of the two preliminary linear regression models, the more desirable variable is average pace, as it has stronger relationships with other variables. 

```{r}
# Create preliminary model

prelim_spd <- lm(avg_spd ~ ., df)
summary(prelim_spd)

```

```{r}
prelim_pace <- lm(avg_pace_sec ~ ., df)
summary(prelim_pace)

```

The ultimate goal of this model is to utilize data leading up to a performance event. As many races take place on Sunday and the typical long-distance run in this data set takes place on Sunday, the final linear regression model will begin with predicting Sunday performance.

To being predicting run performance, an initial linear regression model will be built below using all available data. Based on the preliminary linear regression above, an aerobic training effect that has a high impact (value between 4 and 4.9) is strongly related to average pace. This variable will be the target variable in the logistic regression that follows. 

```{r}
set.seed(456)
# Split data into training and testing sets
df_split <- initial_split(df, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
pace_rec <- recipe(avg_hr ~ ., data = train_df)

summary(pace_rec)
```

```{r}
lm_pace <- linear_reg() %>%
  set_engine("lm")

pace_wflow <- workflow()%>%
  add_model(lm_pace) %>%
  add_recipe(pace_rec)

pace_fit <- pace_wflow %>% 
  fit(data = train_df) 

tidy(pace_fit)
```

```{r}
predict(pace_fit, test_df)
```

```{r}
pace_aug <- augment(pace_fit, test_df)

pace_aug %>% select(avg_pace_sec, .pred)
```

The R Mean-Squared Error for this model is 5.41. In other words, this model can predict average pace within 5.41 seconds.

```{r}
pace_error <- pace_aug %>% 
  rmse(truth = avg_pace_sec, .pred)

pace_error
```

The most significant variables (based on p-value) are average heart rate, average cadence, average stride, and aerobic training effect. The binary variable aerobic_fct_Impacting had a good p-value, as well, but that value is related to aerobic training effect, so it is left out of this analysis. As an attempt to improve the quality of the model, only the variables with the highest p-values will be included in this analysis.
```{r}
set.seed(456)
# Split data into training and testing sets
df_split <- initial_split(df, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
pace_rec_2 <- recipe(avg_pace_sec ~ avg_hr + avg_run_cadence + avg_stride + aerobic_TE, data = train_df)

summary(pace_rec_2)
```

```{r}
lreg <- linear_reg() %>%
  set_engine("lm")

pace_wflow_2 <- workflow()%>%
  add_model(lreg) %>%
  add_recipe(pace_rec_2)

pace_fit_2 <- pace_wflow_2 %>% 
  fit(data = train_df) 

tidy(pace_fit_2)
```

```{r}
predict(pace_fit_2, test_df)
```

```{r}
pace_aug_2 <- augment(pace_fit_2, test_df)

pace_aug_2 %>% select(avg_pace_sec, .pred)
```

Reviewing the results, the quality of the model decreased slightly. However, it seems that average pace will be a good target variable in exploring performance improvements. 

```{r}
pace_error_2 <- pace_aug_2 %>% 
  rmse(truth = avg_pace_sec, .pred)

pace_error_2
```


## Logistic Regression

### All Variables

This first logistic regression is meant to predict whether an activity highly impacts aerobic training. This variable is relevant because it is the highest measure for aerobic conditioning without being over-reaching. In this analysis, calories and other variables related to aerobic training effect were removed. 
```{r}
set.seed(456)

df2 <- df %>% mutate(high_impact = ifelse(aerobic_fct == "Highly Impacting", 1,0))
df2$high_impact <- factor(df2$high_impact)

# Split data into training and testing sets
df2_split <- initial_split(df2, prop = 3/4)

train_df2 <- training(df2_split)
test_df2 <- testing(df2_split)

# Create recipe. Use all variables except aerobic_TE and related
aerobic_rec <- recipe(high_impact ~ short_distance + middle_distance + long_distance + 
                        max_spd + avg_spd + anaerobic_value + `sweat_loss(ml)` + best_pace_sec +
                        avg_pace_sec + max_elevation + min_elevation + avg_stride +
                        total_decent + total_ascent + max_run_cadence + avg_run_cadence + 
                        max_hr + avg_hr + distance, data = train_df2)

summary(aerobic_rec)
```

```{r}
log_reg <- logistic_reg() %>%
  set_engine("glm")

aero_wkfl <- workflow()%>%
  add_model(log_reg) %>%
  add_recipe(aerobic_rec)

aero_fit <- aero_wkfl %>% 
  fit(data = train_df2) 

tidy(aero_fit)
```

```{r}
predict(aero_fit, test_df2)
```

```{r}
aero_aug <- augment(aero_fit, test_df2)

aero_aug %>% select(high_impact, .pred_class)
```

```{r}
aero_aug$.pred_class <- as.character(aero_aug$.pred_class)
aero_aug$.pred_class <- as.numeric(aero_aug$.pred_class)

aero_aug %>% 
  roc_curve(truth = high_impact, .pred_class, event_level="second") %>%
  autoplot()
```

```{r}
aero_aug %>% 
  roc_auc(truth = high_impact, .pred_class, event_level="second")
```

The first logistic regression is a good predictive model. The next step is to select fewer variables to see those increase the reliability of the model. 

```{r}
# Create recipe. Use all variables except aerobic_TE and related
aerobic_rec2 <- recipe(high_impact ~ middle_distance + avg_spd + min_elevation + avg_stride +
                       avg_run_cadence + avg_hr, data = train_df2)

summary(aerobic_rec2)
```

```{r}
log_reg <- logistic_reg() %>%
  set_engine("glm")

aero_wkfl2 <- workflow()%>%
  add_model(log_reg) %>%
  add_recipe(aerobic_rec2)

aero_fit2 <- aero_wkfl2 %>% 
  fit(data = train_df2) 

tidy(aero_fit2)
```

```{r}
predict(aero_fit2, test_df2)
```

```{r}
aero_aug2 <- augment(aero_fit2, test_df2)

aero_aug2 %>% select(high_impact, .pred_class)
```

```{r}
aero_aug2$.pred_class <- as.character(aero_aug2$.pred_class)
aero_aug2$.pred_class <- as.numeric(aero_aug2$.pred_class)

aero_aug2 %>% 
  roc_curve(truth = high_impact, .pred_class, event_level = "second") %>%
  autoplot()
```

```{r}
aero_aug2 %>% 
  roc_auc(truth = high_impact, .pred_class, event_level = "second")
```

Both of these models are acceptable for predicting whether a run highly impacts performance. These analyses provide a good starting point for building a more complex model that can predict good performance. The possible next step is to use k-fold cross validation to predict when good performance will happen given a series of events. 

Thinking more about the aim of this project, the goal is to predict the quality of a workouts without favoring workouts that are fast. In a distance running training plan, a good-quality workout could be a lactate threshold run at race pace, or it could be a recovery run that is two or three minutes slower than race pace. One target variable that can account for these two types of workouts (and the spectrum of workouts in between) is heart rate. The idea here is that a recovery run would have a much lower heart rate, while a threshold run would have a higher heart rate. when more observations are available, many models for different workout types could be deployed (ex. one model for threshold runs, one model for easy runs, one model for recovery runs, etc). Having a target average heart rate set going into a workout would be beneficial for athletes without coaches who need to strike a balance between quality training sessions and preventing over- or under-training. 

```{r}
prelim_hr <- lm(avg_hr ~ ., df)
summary(prelim_hr)
```

In this section, the random forest model will use k-fold cross validation and train with all variables. After more consideration about the project aims, average pace was selected as the target variable. Strictly using the aerobic training effect variable would essentially just copy the work Garmin already does. 
```{r}
pacman::p_load(tidymodels, ranger, parallel)

cores <- parallel::detectCores()

set.seed(456)

# Split data into training and testing sets
df_split <- initial_split(df, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
rf_rec <- recipe(avg_hr ~ ., data = train_df) %>%
          step_dummy(all_nominal_predictors())

folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_hr)

summary(rf_rec)
```

```{r}
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_res <- rf_wf %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res %>%
  show_best(metric = "rmse")

autoplot(rf_res)
```

```{r}
rf_best <- rf_res %>%
  select_best(metric = "rmse")
rf_res %>% collect_predictions()
```

```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(rf_best)

final_fit_rf <- final_rf_wf %>%
  last_fit(df_split)

final_fit_rf %>% collect_metrics()

rf_rmse <- 
  rf_res %>%
  collect_predictions(parameters = rf_best) %>%
  rmse(avg_hr, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse
```

This model predicts average pace within 3.2 seconds. In an attempt to improve the accuracy, some variables will be eliminated. This model ran with 21 predictors. The next iteration will eliminate four predictors: max_hr(highly-correlated with avg_hr), min_elevation, max_elevation, and sweat_loss(ml).

```{r}
set.seed(456)

#get rid of max_hr,min_elevation, max_elevation, and sweat_loss
df1 <- df %>% select(-max_hr, -min_elevation, -max_elevation, -`sweat_loss(ml)`)

# Split data into training and testing sets
df1_split <- initial_split(df1, prop = 3/4)

train_df1 <- training(df1_split)
test_df1 <- testing(df1_split)

# Create recipe
rf_rec <- recipe(avg_hr ~ ., data = train_df1) %>%
          step_dummy(all_nominal_predictors())

folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_hr)

summary(rf_rec)
```

```{r}
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_res <- rf_wf %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res %>%
  show_best(metric = "rmse")

autoplot(rf_res)
```

```{r}
rf_best <- rf_res %>%
  select_best(metric = "rmse")
rf_res %>% collect_predictions()
```

```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(rf_best)

final_fit_rf <- final_rf_wf %>%
  last_fit(df1_split)

final_fit_rf %>% collect_metrics()

rf_rmse <- 
  rf_res %>%
  collect_predictions(parameters = rf_best) %>%
  rmse(avg_hr, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse
```

The accuracy (measured by RMSE) improved slightly to 3.17. Try dropping more variables. This time, ascent, descent, aerobic factors and anaerobic factors. 


Try running the model with tuned parameters.
```{r}
set.seed(456)

#get rid of max_hr,min_elevation, max_elevation, and sweat_loss
df1 <- df %>% select(-max_hr, -min_elevation, -max_elevation, -`sweat_loss(ml)`)

# Split data into training and testing sets
df1_split <- initial_split(df1, prop = 3/4)

train_df1 <- training(df1_split)
test_df1 <- testing(df1_split)

# Create recipe
rf_rec <- recipe(avg_hr ~ ., data = train_df1) %>%
          step_dummy(all_nominal_predictors())

folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_hr)

summary(rf_rec)
```

```{r}
rf_mod <- rand_forest(mtry = 6, min_n = 7, trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_res <- rf_wf %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res %>%
  show_best(metric = "rmse")
```

```{r}
rf_best <- rf_res %>%
  select_best(metric = "rmse")
rf_res %>% collect_predictions()
```

```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(rf_best)

final_fit_rf <- final_rf_wf %>%
  last_fit(df1_split)

final_fit_rf %>% collect_metrics()

rf_rmse <- 
  rf_res %>%
  collect_predictions(parameters = rf_best) %>%
  rmse(avg_hr, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse
```

This final Random Forest model has a greater RMSE value than the previous one. One of the concerns with the dataset is the greater number of features (21 total predictors avialable). LASSO may be a good option to automate feature selection. 

```{r}
set.seed(456)
# Split data into training and testing sets
df_split <- initial_split(df, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
lasso_rec <- recipe(avg_hr ~ ., data = train_df)

# create folds
folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_hr)

summary(lasso_rec)
```

```{r}
lasso_mod <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("lm")

lasso_wkfl <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(lasso_rec)
```

```{r}
# create penalty grid for tuning
lasso_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

# lowest penalties
lasso_grid %>% top_n(-5)

#highest penalties
lasso_grid %>% top_n(5)
```

```{r}
lasso_res <- lasso_wkfl %>%
  tune_grid(folds,
            grid = lasso_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
top_models <- lasso_res %>%
  show_best("rmse")
top_models
```

```{r}
lasso_best <- lasso_res %>%
  select_best("rmse")
lasso_best

final_lasso_wf <- lasso_wkfl %>%
  finalize_workflow(lasso_best)

final_lasso_fit <- final_lasso_wf %>%
  last_fit(df_split)

final_lasso_fit %>% collect_metrics()
```

The LASSO model does improve the RMSE and is likely the best path forward. The next steps in this project will be to further tune this model. 