---
title: "Modeling Run Performance"
author: "Joe Martin"
date: "12/9/2021"
output: pdf_document
---
## Garmin Data Modeling

```{r echo=FALSE}
# Load packages and data
pacman::p_load(tidyverse, tidymodels, here)

garmin <- read_rds(here::here("data","processed_data", "garmin_data.rds"))

# Transform data to have only the necessary variables
df <- garmin %>% drop_na()
```

The target variable is average pace (avg_pace), but I will also compare Average Speed (avg_speed) in miles per hour. A higher average speed and a lower average pace are the desired outcome when measuring performance over time. Reviewing the results of the two preliminary linear regression models, the more desirable variable is average pace, as it has stronger relationships with other variables. 

The target variable is average pace (avg_pace_sec), measured in seconds. Average pace is the best variable to use because it can predict race times, but be applied to different race lengths. Additionally, it is an actionable measure. A runner can easily monitor and control their pace using a fitness watch. It is important that this model has low error. Even a small amount of error could amount to a dramatic difference in final race time. For example, if a person runs a marathon at a 7:00 pace, their final time is 3:03:32. If a second athlete runs a marathon at a 7:05 pace, they would achieve a 3:05:43 marathon. From this example, we can see that a runner hoping to qualify for a race like the Boston Marathon with a 3:05:00 time would be at risk if they are off pace by just 5 seconds per mile. This will be the benchmark for RMSE values - an estimated value of less than 5 seconds per mile.  
```{r}
# Create preliminary test
prelim_spd <- lm(avg_spd ~ ., df)
prelim_spd_table <- summary(prelim_spd)

#write.table(prelim_spd_table, file = here::here("figures","prelim_spd_table"))
```

```{r}
prelim_pace <- lm(avg_pace_sec ~ ., df)
summary(prelim_pace)
```

The ultimate goal of this model is to utilize data leading up to a performance event. Thinking about the purpose of the model (predicting how well I can perform given a set of racing conditions), the best target variable to choose is Average Pace (using only seconds as the unit). This variable is easier to work with than total time (which is in an HMS format) while having the same outcome. It is also something I can know in real-time on runs through my watch and has actionable meaning, compared to the average speed variable. Going forward, all models will use average pace (in seconds) as the target variable and use a linear regression for prediction.

```{r}
set.seed(456)
# Split data into training and testing sets
df_split <- initial_split(df, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
pace_rec <- recipe(avg_pace_sec ~ ., data = train_df)

summary(pace_rec)
```

```{r}
lm_pace <- linear_reg() %>%
  set_engine("lm")

pace_wflow <- workflow()%>%
  add_model(lm_pace) %>%
  add_recipe(pace_rec)

pace_fit <- pace_wflow %>% 
  fit(data = train_df) 

tidy(pace_fit)
```

```{r}
predict(pace_fit, test_df)
```

```{r}
pace_aug <- augment(pace_fit, test_df)

pace_aug %>% select(avg_pace_sec, .pred)
```

The R Mean-Squared Error for this model is 5.64. In other words, this model can predict average pace within 5.24 seconds.

```{r}
pace_error <- pace_aug %>% 
  rmse(truth = avg_pace_sec, .pred)

pace_error
```

These analyses provide a good starting point for building a more complex model that can predict good performance. The possible next step is to use v-fold cross validation to enhance the quality of my training set. In this section, the random forest model will use v-fold cross validation and train with all variables.
```{r}
pacman::p_load(tidymodels, ranger, parallel)

cores <- parallel::detectCores()

set.seed(456)

# Split data into training and testing sets
df_split <- initial_split(df, prop = 3/4)

train_df <- training(df_split)
test_df <- testing(df_split)

# Create recipe
rf_rec <- recipe(avg_pace_sec ~ ., data = train_df) %>%
          step_dummy(all_nominal_predictors())

folds <- vfold_cv(train_df, v = 10, repeats = 5, strata = avg_pace_sec)

summary(rf_rec)
```

```{r}
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_res <- rf_wf %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res %>%
  show_best(metric = "rmse")

autoplot(rf_res)
```

```{r}
rf_best <- rf_res %>%
  select_best(metric = "rmse")
rf_res %>% collect_predictions()
```

```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(rf_best)

final_fit_rf <- final_rf_wf %>%
  last_fit(df_split)

final_fit_rf %>% collect_metrics()

rf_rmse <- 
  rf_res %>%
  collect_predictions(parameters = rf_best) %>%
  rmse(avg_pace_sec, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse
```

Next, tune the parameters. mtry = 24, min_n=3

```{r}
tuned_rf <- rand_forest(mtry = 24, min_n = 3, trees = 1000) %>%
  set_engine("ranger", num.threads = cores, importance = "impurity") %>%
  set_mode("regression")

tuned_wf <- rf_wf %>%
  update_model(tuned_rf)

tuned_rf_fit <- tuned_wf %>%
  last_fit(df_split)
tuned_rf_fit

tuned_rf_fit %>% collect_metrics()
```

This model predicts average pace within 2.45 seconds. This is an excellent error value, given the constraints defined earlier. Because this error is lower than 5 seconds per mile, it would work well as a final model. 

One more consideration to try improving this model is that there is a large number of features, a total of 21 predictors. The following figure shows how which are most relevant to predicting average pace:
```{r}
pacman::p_load(vip)
tuned_rf_fit %>%
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>%
  vip(num_features = 21)
```
Reviewing these relevance of each variable, it seems that the variable with the greatest impact is average speed (avg_spd). When building the model, the `importance = "impurity"` argument sets the importance measurement to variance by default for regression models. This figure is problematic because it the avg_spd variable may constitute data leakage. Technically, this value is not known until the conclusion of a run and it is directly related to the target variable. The model should be re-run without avg_spd. 

```{r}
pacman::p_load(tidymodels, ranger, parallel)

cores <- parallel::detectCores()

set.seed(456)

no_spd <- df %>% select(-avg_spd, -max_spd)

# Split data into training and testing sets
df_split2 <- initial_split(no_spd, prop = 3/4)

train_df2 <- training(df_split2)
test_df2 <- testing(df_split2)

# Create recipe
rf_rec2 <- recipe(avg_pace_sec ~ ., data = train_df2) %>%
          step_dummy(all_nominal_predictors())

folds <- vfold_cv(train_df2, v = 10, repeats = 5, strata = avg_pace_sec)

summary(rf_rec2)
```

```{r}
rf_mod2 <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf2 <- workflow() %>%
  add_model(rf_mod2) %>%
  add_recipe(rf_rec2)

rf_res2 <- rf_wf2 %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res2 %>%
  show_best(metric = "rmse")

rf_res2_table <- rf_res2 %>%
  show_best(metric = "rmse") 
saveRDS(rf_res2_table, here::here("figures","rf_res2_table.Rds"))

rf_res2_plot <- autoplot(rf_res2)
ggsave(here::here("figures","random_forest_res2_plot.png"), device = "png", scale = 3)
```

```{r}
rf_best2 <- rf_res2 %>%
  select_best(metric = "rmse")
rf_res2 %>% collect_predictions()

rf_best2
```

```{r}
rf_res2 %>% collect_predictions %>%
  ggplot(aes(x = avg_pace_sec, y = .pred , color = id2))+
  geom_point(alpha = .3)+
  geom_abline()+
  theme_classic()+
  labs(x = "Truth",
       y = "Predicted Average Pace")
ggsave(here::here("figures","predictions_rand_forest2.png"), device = "png", scale = 3)
```

```{r}
final_rf_wf2 <- rf_wf2 %>%
  finalize_workflow(rf_best2)

final_fit_rf2 <- final_rf_wf2 %>%
  last_fit(df_split2)

final_fit_rf2 %>% collect_metrics()

final_fit_rf2 %>%
  collect_metrics()

rf_rmse2 <- 
  final_fit_rf2 %>%
  collect_predictions(parameters = rf_best2) %>%
  rmse(avg_pace_sec, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse2
```

Tune parameters. mtry = 22, min_n = 3
```{r}
tuned_rf2 <- rand_forest(mtry = 22, min_n = 3, trees = 1000) %>%
  set_engine("ranger", num.threads = cores, importance = "impurity") %>%
  set_mode("regression")

tuned_wf2 <- rf_wf2 %>%
  update_model(tuned_rf2)

tuned_rf_fit2 <- tuned_wf2 %>%
  last_fit(df_split2)
tuned_rf_fit2

rf2_fit_table <- tuned_rf_fit2 %>% collect_metrics()
saveRDS(rf2_fit_table, here::here("figures","rf2_fit_table.Rds"))
```

```{r}
tuned_rf_fit2 %>%
  unnest(.predictions) %>%
  ggplot(aes(avg_pace_sec, .pred, color = id)) +
  geom_abline() +
  geom_point(alpha = 0.5) +
  theme_classic()+
  theme(legend.position = "none")+
  labs(
    x = "Truth",
    y = "Predicted Average Pace"
  )
ggsave(here::here("figures","random_forest_predictions2.png"), device = "png", scale = 3)
```

```{r}
tuned_rf_fit2 %>%
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>%
  vip(num_features = 21)
```

Try random forest one more time without unimportant variables.

```{r}
set.seed(456)
# Split data into training and testing sets
# use no_spd splits called df_split2, train_df2, and test_df2

#new dataframe
final_df <- no_spd %>% select(-anaerobic_fct,-aerobic_fct,-max_elevation,-rhr,-max_run_cadence, -anaerobic_value, -short_distance, -long_distance,-middle_distance)

final_split <- initial_split(final_df, prop = 3/4, strata = avg_pace_sec)

train_fin <- training(final_split)
testing_fin <- testing(final_split)

# Create recipe
rf_rec3 <- recipe(avg_pace_sec ~ ., data = train_fin)%>%
          step_dummy(all_nominal_predictors())
rf_rec3

# create folds
folds <- vfold_cv(train_fin, v = 10, repeats = 5, strata = avg_pace_sec)

```

```{r}
# Use the same spec
rf_mod2 <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf3 <- workflow() %>%
  add_model(rf_mod2) %>%
  add_recipe(rf_rec3)

rf_res3 <- rf_wf3 %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

```{r}
rf_res3_table <- rf_res3 %>%
  show_best(metric = "rmse")
rf_res3_table

saveRDS(rf_res3_table, here::here("figures","rf_res3_table.Rds"))


rf_res3_plot <- autoplot(rf_res3)
ggsave(here::here("figures","random_forest_res3_plot.png"), device = "png", scale = 3)
```

```{r}
rf_best3 <- rf_res3 %>%
  select_best(metric = "rmse")
rf_res3 %>% collect_predictions()

rf_best3
```

```{r}
rf_res3 %>% collect_predictions %>%
  ggplot(aes(x = avg_pace_sec, y = .pred , color = id2))+
  geom_point(alpha = .3)+
  geom_abline()+
  theme_classic()+
  labs(x = "Truth",
       y = "Predicted Average Pace")
ggsave(here::here("figures","predictions_rand_forest3.png"), device = "png", scale = 3)
```

```{r}
final_rf_wf3 <- rf_wf3 %>%
  finalize_workflow(rf_best3)

final_fit_rf3 <- final_rf_wf3 %>%
  last_fit(final_split)

rf3_fit_table <- final_fit_rf3 %>% collect_metrics()
rf3_fit_table
saveRDS(rf3_fit_table, here::here("figures", "rf3_fit_table.Rds"))

rf_rmse3 <- 
  final_fit_rf3 %>%
  collect_predictions(parameters = rf_best3) %>%
  rmse(avg_pace_sec, .pred) %>%
  mutate(model = "Random Forest")
rf_rmse3
```

Tune parameters. mtry = 8, min_n = 4
```{r}
tuned_rf3 <- rand_forest(mtry = 8, min_n = 4, trees = 1000) %>%
  set_engine("ranger", num.threads = cores, importance = "impurity") %>%
  set_mode("regression")

tuned_wf3 <- rf_wf3 %>%
  update_model(tuned_rf3)

tuned_rf_fit3 <- tuned_wf3 %>%
  last_fit(final_split)
tuned_rf_fit3

tuned_rf3_fit_table <- tuned_rf_fit3 %>% collect_metrics()
saveRDS(tuned_rf3_fit_table, here::here("figures","tuned_rf3_fit_table.Rds"))
```

```{r}
tuned_rf_fit3 %>%
  unnest(.predictions) %>%
  ggplot(aes(avg_pace_sec, .pred, color = id)) +
  geom_abline() +
  geom_point(alpha = 0.5) +
  theme_classic()+
  theme(legend.position = "none")+
  labs(
    x = "Truth",
    y = "Predicted Average Pace"
  )
ggsave(here::here("figures","random_forest_predictions3.png"), device = "png", scale = 3)
```

```{r}
tuned_rf_fit3 %>%
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>%
  vip(num_features = 21)
```
The least-important features did have an impact on the quality of the model. Therefore, the initial random forest will be selected out of the initial few models. 

The RMSE for this model is still not as good as I had hoped, but this shows a clearer picture of which variables are most important. It seems that many of the variables have little affect on the model. With the relatively large number of variables in this model, a LASSO regression may be a good option to automate feature selection. 

```{r}
# Excellent tidymodels LASSO tutorial from Julia Silge: https://www.youtube.com/watch?v=R32AsuKICAY
set.seed(456)
# Split data into training and testing sets
# use no_spd splits called df_split2, train_df2, and test_df2

#new dataframe
final_df <- no_spd %>% select(-anaerobic_fct,-aerobic_fct,-max_elevation,-rhr,-max_run_cadence, -anaerobic_value, -short_distance, -long_distance,-middle_distance)

final_split <- initial_split(final_df, prop = 4/5, strata = avg_pace_sec)

train_fin <- training(final_split)
testing_fin <- testing(final_split)

# Create recipe
lasso_rec <- recipe(avg_pace_sec ~ ., data = train_fin) %>%
  step_zv(all_numeric(), -all_outcomes()) %>%
  step_normalize(all_numeric(), -all_outcomes()) #center and scale
lasso_rec

# create folds
folds <- vfold_cv(train_df2, v = 10, repeats = 5, strata = avg_pace_sec)

# create validation set
val_set <- validation_split(train_df2, 
                            strata = avg_pace_sec, 
                            prop = 0.80)
val_set

summary(lasso_rec)
```

```{r}
lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet")
```

```{r}
lasso_wkfl <- workflow() %>%
  add_recipe(lasso_rec)

lasso_fit <- lasso_wkfl %>%
  add_model(lasso_spec) %>%
  fit(data = train_fin)

lasso_fit %>%
  pull_workflow_fit() %>%
  tidy()
```

```{r}
# pick the penalty value with resampling and tuning
# when running models, I keep getting the warning "! Bootstrap11: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading"
# Upon further research, it seems that I need to remove variables for this to work well. I'm going to get rid of the least important variables based on the previous table. 


set.seed(456)
garmin_boot <- bootstraps(train_fin, strata = avg_pace_sec)

tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lambda_grid <- grid_regular(penalty(),
                            levels = 50)

doParallel::registerDoParallel()

set.seed(2020)
lasso_grid <- tune_grid(
  lasso_wkfl %>%
    add_model(tune_spec),
  resamples = garmin_boot, 
  grid = lambda_grid
)
```

```{r}
lasso_grid_plot <- lasso_grid %>% 
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err),
                alpha = .5)+
  geom_line(show.legent = FALSE) + 
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10()
lasso_grid_plot
ggsave(here::here("figures","lasso_grid_plot.png"), device = "png", scale = 3)
```

```{r}
low_rmse <- lasso_grid %>%
  select_best("rmse") #best metric is model 23

#create final workflow
final_lasso <- finalize_workflow(lasso_wkfl %>%
                    add_model(tune_spec),
                  low_rmse)
pacman::p_load(vip)

#train best model
lasso_vi <- final_lasso %>%
  fit(train_fin) %>%
  pull_workflow_fit() %>%
  vi(lambda = low_rmse$penalty) %>%
  mutate(Importance = abs(Importance),
         Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign))+
  geom_col()
lasso_vi
ggsave(here::here("figures","lasso_vi.png"), device = "png", scale = 3)

```

```{r}
lasso_fit_table <- last_fit(final_lasso,
         final_split) %>%
  collect_metrics()
lasso_fit_table
saveRDS(lasso_fit_table, here::here("figures","lasso_fit_table.png"))
```
After creating a grid for this LASSO model, I found that my RMSE actually did worse. I want it to be under 5, but this resulted in a value greater than 10. The other important lesson learned with this model is that most of the variables with high importance have negative importance values. This means these variables could be irrelevant, or it could mean that my model is underfitting based on these variables. Since LASSO models are used to regularize, I'm going to try another model to see if I can improve my results. Since I had better luck with my random forest, I'm going to return to that and follow a different process laid out by Julia Silge in this tidy tuesday: https://juliasilge.com/blog/intro-tidymodels/. The difference with this model is that I previously used `glmnet` as my model engine. I'm planning now to use `lm` and follow Silge's steps for building a random forest.

```{r}
#create a simple linear model. This will be used to compare to random forest values
set.seed(456)
rf_split <- final_df %>%
  initial_split(strata = avg_pace_sec)

rf_train <- training(rf_split)
rf_test <- testing(rf_split)

# Create recipe
rf_rec <- recipe(avg_pace_sec ~ ., data = train_fin) %>%
  step_zv(all_numeric(), -all_outcomes()) %>%
  step_normalize(all_numeric(), -all_outcomes()) #center and scale
rf_rec

#initiate model
lm_spec <- linear_reg() %>%
  set_engine(engine = "lm")

#fit model
lm_fit <- lm_spec %>%
  fit(avg_pace_sec ~ .,
    data = rf_train
  )
lm_fit
```

```{r}
#set engine
rf_spec <- rand_forest(mode = "regression") %>%
  set_engine("ranger")
rf_spec

#create fit without recipe.
rf_fit <- rf_spec %>%
  fit(avg_pace_sec ~ .,
    data = rf_train
  )

rf_fit
```

```{r}
results_train <- lm_fit %>%
  predict(new_data = rf_train) %>%
  mutate(
    truth = rf_train$avg_pace_sec,
    model = "lm"
  ) %>%
  bind_rows(rf_fit %>%
    predict(new_data = rf_train) %>%
    mutate(
      truth = rf_train$avg_pace_sec,
      model = "rf"
    ))

results_test <- lm_fit %>%
  predict(new_data = rf_test) %>%
  mutate(
    truth = rf_test$avg_pace_sec,
    model = "lm"
  ) %>%
  bind_rows(rf_fit %>%
    predict(new_data = rf_test) %>%
    mutate(
      truth = rf_test$avg_pace_sec,
      model = "rf"
    ))
```

This model meets the standard of predicting with an RMSE lower than 5. Will the testing data work, as well?
```{r}
results_train %>%
  group_by(model) %>%
  rmse(truth = truth, estimate = .pred)
```

This model still is not a great choice based on the RMSE value for the testing data. The next step is to try resampling. 
```{r}
results_test %>%
  group_by(model) %>%
  rmse(truth = truth, estimate = .pred)
```
Once again, the final model did not perform as well as expected. 
```{r}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
    mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = model)) +
  geom_abline() +
  geom_point(alpha = 0.5) +
  facet_wrap(~train) +
  labs(
    x = "Truth",
    y = "Predicted attendance",
    color = "Type of model"
  )
```   


```{r eval = FALSE}
# training
set.seed(456)
rf_folds <- rsample::vfold_cv(rf_train)

rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rf_rec) 
  

rf_res <- rf_wf %>% fit_resamples(
  resamples = (rf_folds),
  control = control_resamples(save_pred = TRUE)
)

rf_res %>%
  collect_metrics()

#testing
rf_testing_fit <- predict(rf_wf, testing_fin)

rf_final <- rf_wf %>%
  last_fit(final_split)
rf_final #fitting test data using resampled results

rf_final %>% collect_metrics()
```

Once again, the final fit did not do as well as the initial fit. 
```{r}
rf_res %>%
  unnest(.predictions) %>%
  ggplot(aes(avg_pace_sec, .pred, color = id)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Truth",
    y = "Predicted game attendance",
    color = NULL
  )

```
As it turns out, the best model built so far was the simple regression. This, however, contained the average speed variable I eliminated because I think it causes data leakage. Therefore, I'm going to try rebuilding this model to see if I can continue to get good results with a linear regression. 

```{r}
set.seed(456)
# use final split (final_split, train_fin, testing_fin)

# Create recipe
pace_rec_fin <- recipe(avg_pace_sec ~ ., data = train_fin)

summary(pace_rec_fin)
```

```{r}
lm_pace <- linear_reg() %>%
  set_engine("lm")

pace_wflow_fin <- workflow()%>%
  add_model(lm_pace) %>%
  add_recipe(pace_rec_fin)

pace_fit_fin <- pace_wflow_fin %>% 
  fit(data = train_fin) 

tidy(pace_fit_fin)
```

```{r}
predict(pace_fit_fin, testing_fin)
```

```{r}
pace_aug_fin <- augment(pace_fit_fin, testing_fin)

pace_aug_fin %>% select(avg_pace_sec, .pred)
```

This model did not perform as well as I had hoped. Removing the average speed variable dramatically changes how the model behaves. The best model, therefore, is the tuned random forest. This is the model I will deploy. 

```{r}
pace_error_fin <- pace_aug_fin %>% 
  rmse(truth = avg_pace_sec, .pred)

pace_error_fin
```